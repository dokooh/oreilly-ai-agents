{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CRAG - Corrective Retrieval Augmented Generation\n",
    "\n",
    "Inspiration: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-***'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup / Grabbing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-47NGgZAvFz-",
    "outputId": "72f0e0c9-0fa5-45b2-fd88-f3c9f710afa5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxTG7Kk2xZhm",
    "outputId": "e08810b4-d44d-4fb6-d3e7-6dad5c406ad8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/9xqbqkg90tnc0cmm0dxt985m0000gn/T/ipykernel_65474/2952994269.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "Few-shot CoT. It is to prompt the model with a few ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "Zero-shot generation: This is to find a number of  ... https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\n",
      "Text: i'll bet the video game is a lot more fun th ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n"
     ]
    }
   ],
   "source": [
    "question = \"What is few-shot learning?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:50], '...', doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhapNmaowfp-",
    "outputId": "98a6405d-d1bb-49bb-93a1-cb99e4c2636f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes' Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "binary_score='no' Few-shot CoT. It is to prompt the model with a few ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "binary_score='no' Zero-shot generation: This is to find a number of  ... https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\n",
      "binary_score='no' Text: i'll bet the video game is a lot more fun th ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "for doc in docs:\n",
    "    doc_txt = doc.page_content\n",
    "    print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}), doc_txt[:50], '...', doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2X8QeJkxye7",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate Compoments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yfm8E7gvwu3C",
    "outputId": "8d6e0406-b041-448c-eaad-9cefbb745fc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinanozdemir/Library/Python/3.9/lib/python/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "for message in prompt.messages:\n",
    "    print(type(message))\n",
    "    print(message.prompt.template)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUzV8hwgx0G0",
    "outputId": "78fd6604-2e40-4979-992e-c79323472b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot learning is a machine learning approach where a model is provided with a small number of high-quality examples, each containing both input and desired output, to better understand the task at hand. This method helps the model grasp human intentions and criteria for responses, often resulting in improved performance compared to zero-shot learning. However, it requires more tokens and can risk hitting context length limits with longer inputs and outputs.\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question Re-writer / The Corrective Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rX5WDbN6z010",
    "outputId": "b558819c-b304-4107-c5ce-320b4de76d03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is few-shot learning?',\n",
       " 'What are the key concepts and applications of few-shot learning in machine learning?')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A different LLM, just to show we can use multiple LLMs in our calls\n",
    "three_five_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | three_five_llm | StrOutputParser()\n",
    "question, question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXf2XKOKz1AW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9MlJ26oXzJqw"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        times_transformed: number of times the question has been re-written\n",
    "        web_search: if we should be doing a web search (not implemented in this notebook)\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    times_transformed: int\n",
    "    web_search: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def set_state(state):\n",
    "    \"\"\"\n",
    "    Sets initial state\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---SET STATE---\")\n",
    "\n",
    "    return {\"times_transformed\": 0}\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    print(state)\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": format_docs(documents), \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    times_transformed = state[\"times_transformed\"]\n",
    "    times_transformed += 1\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print('---NEW QUESTION---')\n",
    "    print(better_question)\n",
    "    return {\"question\": better_question, \"times_transformed\": times_transformed}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        print(d.metadata['source'], f'Grade: {grade}')\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "    if len(filtered_docs) == 0:\n",
    "        print(\"---GRADE: DOCUMENTS NOT RELEVANT---\")\n",
    "        web_search = \"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dE99Sfa-zOvA"
   },
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    # state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # check times_transformed\n",
    "        if state[\"times_transformed\"] >= 3:\n",
    "            print(\n",
    "                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION AND WE HAVE TRANSFORMED 3 TIMES, GENERATE---\"\n",
    "            )\n",
    "            return \"should_generate\"\n",
    "\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"should_transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"should_generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z2wbXWAAzm1k"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"set_state\", set_state)  # set_state\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"set_state\")\n",
    "workflow.add_edge(\"set_state\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"should_transform_query\": \"transform_query\",\n",
    "        \"should_generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJ_wjx_M3Db-",
    "outputId": "6ab8e986-702b-4a26-b4ca-74a0ec593d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'What on earth is few shot learning?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      "Few-shot learning is a method where a model is provided with a small number of high-quality examples that include both input and desired output for a specific task. This approach helps the model better understand human intentions and improves performance compared to zero-shot learning. However, it requires more tokens and may reach context length limits with longer texts.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What on earth is few shot learning?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPsnn1CG5QHG",
    "outputId": "8f9299f3-22d4-4101-e7dc-9dbecdd84fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'How to make good inputs to AI?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "How can I optimize input data for artificial intelligence systems?\n",
      "Node 'transform_query':\n",
      "{'question': 'How can I optimize input data for artificial intelligence systems?', 'documents': [], 'times_transformed': 1, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      "To optimize input data for AI systems, first generate knowledge about the topic using internal retrieval methods. This involves extracting relevant information and insights before processing the input. Additionally, ensure the data is clean, well-structured, and representative of the problem domain.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"How to make good inputs to AI?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0J0TtIcb0CYf",
    "outputId": "ebe8c55a-a6a1-47d3-bd87-194d2a972d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'How big is the moon?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "What is the size of the moon in terms of diameter and circumference?\n",
      "Node 'transform_query':\n",
      "{'question': 'What is the size of the moon in terms of diameter and circumference?', 'documents': [], 'times_transformed': 1, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "What are the diameter and circumference measurements of the moon?\n",
      "Node 'transform_query':\n",
      "{'question': 'What are the diameter and circumference measurements of the moon?', 'documents': [], 'times_transformed': 2, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "What are the specific measurements of the moon's diameter and circumference?\n",
      "Node 'transform_query':\n",
      "{'question': \"What are the specific measurements of the moon's diameter and circumference?\", 'documents': [], 'times_transformed': 3, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION AND WE HAVE TRANSFORMED 3 TIMES, GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      "The moon's diameter is approximately 3,474 kilometers (2,159 miles), and its circumference is about 10,921 kilometers (6,786 miles).\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"How big is the moon?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Library/Developer/CommandLineTools/usr/bin/python3'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: grandalf in /Users/sinanozdemir/Library/Python/3.9/lib/python/site-packages (0.8)\n",
      "Collecting pygraphviz\n",
      "  Downloading pygraphviz-1.11.zip (120 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing in /Users/sinanozdemir/Library/Python/3.9/lib/python/site-packages (from grandalf) (3.1.2)\n",
      "Building wheels for collected packages: pygraphviz\n",
      "  Building wheel for pygraphviz (setup.py) ... \u001b[?2error\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[54 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /Users/sinanozdemir/Library/Python/3.9/lib/python/site-packages/setuptools/_distutils/dist.py:270: UserWarning: Unknown distribution option: 'tests_require'\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg)\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-universal2-cpython-39/pygraphviz\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/scraper.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/graphviz.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/agraph.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/testing.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_unicode.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_scraper.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_readwrite.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_string.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/__init__.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_html.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_node_attributes.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_drawing.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_repr_mimebundle.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_subgraph.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_close.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_edge_attributes.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_clear.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_layout.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_attribute_defaults.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/tests/test_graph.py -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz/tests\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing pygraphviz.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to pygraphviz.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to pygraphviz.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'pygraphviz.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.png' under directory 'doc'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.txt' under directory 'doc'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.css' under directory 'doc'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '.svn' found anywhere in distribution\n",
      "  \u001b[31m   \u001b[0m no previously-included directories found matching 'doc/build'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'pygraphviz.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/graphviz.i -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz\n",
      "  \u001b[31m   \u001b[0m copying pygraphviz/graphviz_wrap.c -> build/lib.macosx-10.9-universal2-cpython-39/pygraphviz\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'pygraphviz._graphviz' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-universal2-cpython-39/pygraphviz\n",
      "  \u001b[31m   \u001b[0m clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration -Wno-error=unreachable-code -DSWIG_PYTHON_STRICT_BYTE_CHAR -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9 -c pygraphviz/graphviz_wrap.c -o build/temp.macosx-10.9-universal2-cpython-39/pygraphviz/graphviz_wrap.o\n",
      "  \u001b[31m   \u001b[0m pygraphviz/graphviz_wrap.c:3020:10: fatal error: 'graphviz/cgraph.h' file not found\n",
      "  \u001b[31m   \u001b[0m  3020 | #include \"graphviz/cgraph.h\"\n",
      "  \u001b[31m   \u001b[0m       |          ^~~~~~~~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m 1 error generated.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pygraphviz\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for pygraphviz\n",
      "Failed to build pygraphviz\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pygraphviz)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install grandalf pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "18SWafVC0Yxn",
    "outputId": "55cc9f34-d2e5-4d6c-c60c-726992960383"
   },
   "outputs": [],
   "source": [
    "# Visualize our graph\n",
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                +-----------+                  \n",
      "                | __start__ |                  \n",
      "                +-----------+                  \n",
      "                       *                       \n",
      "                       *                       \n",
      "                       *                       \n",
      "                +-----------+                  \n",
      "                | set_state |                  \n",
      "                +-----------+                  \n",
      "                       *                       \n",
      "                       *                       \n",
      "                       *                       \n",
      "                 +----------+                  \n",
      "                 | retrieve |                  \n",
      "                 +----------+                  \n",
      "                **           **                \n",
      "              **               **              \n",
      "            **                   **            \n",
      "+-----------------+                **          \n",
      "| grade_documents |                 *          \n",
      "+-----------------+..               *          \n",
      "          .          .....          *          \n",
      "          .               ....      *          \n",
      "          .                   ...   *          \n",
      "    +----------+           +-----------------+ \n",
      "    | generate |           | transform_query | \n",
      "    +----------+           +-----------------+ \n",
      "          *                                    \n",
      "          *                                    \n",
      "          *                                    \n",
      "    +---------+                                \n",
      "    | __end__ |                                \n",
      "    +---------+                                \n"
     ]
    }
   ],
   "source": [
    "# if the above fails try this (requires grandalf)\n",
    "\n",
    "print(app.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (/usr/bin/python3)",
   "language": "python",
   "name": "my_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
