{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CRAG - Corrective Retrieval Augmented Generation\n",
    "\n",
    "Inspiration: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_crag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-***'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup / Grabbing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinanozdemir/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/sinanozdemir/Library/Python/3.9/lib/python/site-packages/langchain_community/document_loaders/blob_loaders/file_system.py:6: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.blob_loaders.schema import Blob, BlobLoader\n",
      "/Users/sinanozdemir/Library/Python/3.9/lib/python/site-packages/langchain_community/document_loaders/__init__.py:227: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.youtube import (\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory='./db'\n",
    "    \n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxTG7Kk2xZhm",
    "outputId": "e08810b4-d44d-4fb6-d3e7-6dad5c406ad8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/9xqbqkg90tnc0cmm0dxt985m0000gn/T/ipykernel_15149/2952994269.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n"
     ]
    }
   ],
   "source": [
    "question = \"What is few-shot learning?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:50], '...', doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhapNmaowfp-",
    "outputId": "98a6405d-d1bb-49bb-93a1-cb99e4c2636f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes' Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "binary_score='yes' Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "binary_score='yes' Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
      "binary_score='yes' Zero-shot and few-shot learning are two most basic ... https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with structured output\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader  # llm + prompt\n",
    "\n",
    "for doc in docs:\n",
    "    doc_txt = doc.page_content\n",
    "    print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}), doc_txt[:50], '...', doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2X8QeJkxye7"
   },
   "source": [
    "## Generate Compoments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yfm8E7gvwu3C",
    "outputId": "8d6e0406-b041-448c-eaad-9cefbb745fc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinanozdemir/Library/Python/3.9/lib/python/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "for message in prompt.messages:\n",
    "    print(type(message))\n",
    "    print(message.prompt.template)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUzV8hwgx0G0",
    "outputId": "78fd6604-2e40-4979-992e-c79323472b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot learning involves presenting the model with a set of high-quality demonstrations, each containing both input and desired output, for the target task. This helps the model better understand human intentions and criteria, often leading to improved performance compared to zero-shot learning. However, it requires more tokens and may hit context length limits with long inputs and outputs.\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Re-writer / The Corrective Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rX5WDbN6z010",
    "outputId": "b558819c-b304-4107-c5ce-320b4de76d03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is few-shot learning?',\n",
       " 'What is few-shot learning in machine learning, and how does it differ from traditional learning methods?')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A different LLM, just to show we can use multiple LLMs in our calls\n",
    "bigger_llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0.1)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | bigger_llm | StrOutputParser()\n",
    "question, question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXf2XKOKz1AW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9MlJ26oXzJqw"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        times_transformed: number of times the question has been re-written\n",
    "        web_search: if we should be doing a web search (not implemented in this notebook)\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    times_transformed: int\n",
    "    web_search: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def set_state(state):\n",
    "    \"\"\"\n",
    "    Sets initial state\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---SET STATE---\")\n",
    "\n",
    "    return {\"times_transformed\": 0}\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    print(state)\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": format_docs(documents), \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    times_transformed = state[\"times_transformed\"]\n",
    "    times_transformed += 1\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print('---NEW QUESTION---')\n",
    "    print(better_question)\n",
    "    return {\"question\": better_question, \"times_transformed\": times_transformed}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        print(d.metadata['source'], f'Grade: {grade}')\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "    if len(filtered_docs) == 0:\n",
    "        print(\"---GRADE: DOCUMENTS NOT RELEVANT---\")\n",
    "        web_search = \"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dE99Sfa-zOvA"
   },
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    web_search = state[\"web_search\"]\n",
    "    # state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # check times_transformed\n",
    "        if state[\"times_transformed\"] >= 3:\n",
    "            print(\n",
    "                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION AND WE HAVE TRANSFORMED 3 TIMES, GENERATE---\"\n",
    "            )\n",
    "            return \"should_generate\"\n",
    "\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"should_transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"should_generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "z2wbXWAAzm1k"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"set_state\", set_state)  # set_state\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"set_state\")\n",
    "workflow.add_edge(\"set_state\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"should_transform_query\": \"transform_query\",\n",
    "        \"should_generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJ_wjx_M3Db-",
    "outputId": "6ab8e986-702b-4a26-b4ca-74a0ec593d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'What on earth is few shot learning?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      "Few-shot learning is a method where the model is given a few high-quality examples of input and desired output for a task before being asked to perform it. This helps the model better understand the task and human intentions, often improving performance compared to zero-shot learning. However, it uses more tokens and may hit context length limits with long inputs and outputs.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What on earth is few shot learning?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPsnn1CG5QHG",
    "outputId": "8f9299f3-22d4-4101-e7dc-9dbecdd84fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'How to make good inputs to AI?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: yes\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "https://lilianweng.github.io/posts/2023-06-23-agent/ Grade: no\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      "To make good inputs to AI, provide clear, specific, and well-structured information or questions. This helps the AI understand your request accurately and generate relevant responses. The process involves receiving your input, planning the task, selecting the appropriate model, and executing the task to produce predictions, which I then analyze and explain to you.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"How to make good inputs to AI?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0J0TtIcb0CYf",
    "outputId": "ebe8c55a-a6a1-47d3-bd87-194d2a972d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SET STATE---\n",
      "Node 'set_state':\n",
      "{'question': 'How big is the moon?', 'times_transformed': 0}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "What is the diameter and surface area of the Moon?\n",
      "Node 'transform_query':\n",
      "{'question': 'What is the diameter and surface area of the Moon?', 'documents': [], 'times_transformed': 1, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "What are the diameter and surface area measurements of the Moon?\n",
      "Node 'transform_query':\n",
      "{'question': 'What are the diameter and surface area measurements of the Moon?', 'documents': [], 'times_transformed': 2, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "---TRANSFORM QUERY---\n",
      "---NEW QUESTION---\n",
      "What is the diameter and total surface area of the Moon in kilometers?\n",
      "Node 'transform_query':\n",
      "{'question': 'What is the diameter and total surface area of the Moon in kilometers?', 'documents': [], 'times_transformed': 3, 'web_search': 'Yes'}\n",
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ Grade: no\n",
      "---GRADE: DOCUMENTS NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION AND WE HAVE TRANSFORMED 3 TIMES, GENERATE---\n",
      "Node 'grade_documents':\n",
      "---GENERATE---\n",
      "Node 'generate':\n",
      "The diameter of the Moon is about 3,474 kilometers. Its total surface area is approximately 37.9 million square kilometers.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"How big is the moon?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "# Final generation\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "18SWafVC0Yxn",
    "outputId": "55cc9f34-d2e5-4d6c-c60c-726992960383"
   },
   "outputs": [],
   "source": [
    "# Visualize our graph\n",
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                +-----------+                  \n",
      "                | __start__ |                  \n",
      "                +-----------+                  \n",
      "                       *                       \n",
      "                       *                       \n",
      "                       *                       \n",
      "                +-----------+                  \n",
      "                | set_state |                  \n",
      "                +-----------+                  \n",
      "                       *                       \n",
      "                       *                       \n",
      "                       *                       \n",
      "                 +----------+                  \n",
      "                 | retrieve |                  \n",
      "                 +----------+                  \n",
      "                **           **                \n",
      "              **               **              \n",
      "            **                   **            \n",
      "+-----------------+                **          \n",
      "| grade_documents |                 *          \n",
      "+-----------------+..               *          \n",
      "          .          .....          *          \n",
      "          .               ....      *          \n",
      "          .                   ...   *          \n",
      "    +----------+           +-----------------+ \n",
      "    | generate |           | transform_query | \n",
      "    +----------+           +-----------------+ \n",
      "          *                                    \n",
      "          *                                    \n",
      "          *                                    \n",
      "    +---------+                                \n",
      "    | __end__ |                                \n",
      "    +---------+                                \n"
     ]
    }
   ],
   "source": [
    "# if the above fails try this (requires grandalf)\n",
    "\n",
    "print(app.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (/usr/bin/python3)",
   "language": "python",
   "name": "my_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
